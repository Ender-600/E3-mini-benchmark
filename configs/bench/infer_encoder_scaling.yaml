name: "infer_encoder_scaling"
arch: "encoder"
# Test multiple context lengths to generate latency curve
context_lengths: [128, 256, 512, 1024]
batch_size: 1
num_runs: 3  # Fewer runs per length since we test multiple lengths
warmup_runs: 1

