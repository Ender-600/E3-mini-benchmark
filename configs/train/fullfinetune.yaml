name: "fullfinetune"
fp16: true
grad_checkpointing: true
per_device_train_batch_size: 8
per_device_eval_batch_size: 16
gradient_accumulation_steps: 4
num_train_epochs: 5
learning_rate: 5e-5
weight_decay: 0.01
warmup_ratio: 0.1
max_length: 256
seed: [42, 43, 44]
save_strategy: "epoch"
eval_strategy: "epoch"
logging_steps: 10
save_total_limit: 2
