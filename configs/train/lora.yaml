name: "lora"
fp16: false
grad_checkpointing: false
per_device_train_batch_size: 16
per_device_eval_batch_size: 32
gradient_accumulation_steps: 2
num_train_epochs: 5
learning_rate: 0.00005  # Reduced from 0.0005 to prevent gradient explosion (1/10 of original)
weight_decay: 0.01
warmup_ratio: 0.1
max_length: 256
seed: [42]
save_strategy: "epoch"
eval_strategy: "epoch"
logging_steps: 10
save_total_limit: 2

# LoRA specific
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
# Note: target_modules will be automatically determined based on model architecture
