name: "lora"
fp16: true
grad_checkpointing: true
per_device_train_batch_size: 16
per_device_eval_batch_size: 32
gradient_accumulation_steps: 2
num_train_epochs: 5
learning_rate: 5e-4
weight_decay: 0.01
warmup_ratio: 0.1
max_length: 256
seed: [42, 43, 44]
save_strategy: "epoch"
eval_strategy: "epoch"
logging_steps: 10
save_total_limit: 2

# LoRA specific
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
