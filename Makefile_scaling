# Run inference scaling (different context lengths) for selected models
infer-scaling:
	@echo "Running Inference Scaling Benchmark on selected models..."
	# 1. BART Base
	@echo "1. Benchmarking BART Base scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/bart-base.yaml --bench_cfg configs/bench/infer_seq2seq_scaling.yaml --output_dir results
	# 2. Flan-T5 Base
	@echo "2. Benchmarking Flan-T5 Base scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/flan-t5-base.yaml --bench_cfg configs/bench/infer_seq2seq_scaling.yaml --output_dir results
	# 3. Flan-T5 Large
	@echo "3. Benchmarking Flan-T5 Large scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/flan-t5-large.yaml --bench_cfg configs/bench/infer_seq2seq_scaling.yaml --output_dir results
	# 4. T5 Base
	@echo "4. Benchmarking T5 Base scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/t5-base.yaml --bench_cfg configs/bench/infer_seq2seq_scaling.yaml --output_dir results
	# 5. T5 Large
	@echo "5. Benchmarking T5 Large scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/t5-large.yaml --bench_cfg configs/bench/infer_seq2seq_scaling.yaml --output_dir results
	# 6. GPT-2 Large
	@echo "6. Benchmarking GPT-2 Large scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/gpt2-large.yaml --bench_cfg configs/bench/infer_decoder_scaling.yaml --output_dir results
	# 7. GPT-2 Medium (Standard GPT-2)
	@echo "7. Benchmarking GPT-2 Medium scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/gpt2-medium.yaml --bench_cfg configs/bench/infer_decoder_scaling.yaml --output_dir results
	# 8. Llama 3.2 1B
	@echo "8. Benchmarking Llama 3.2 1B scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/llama-3.2-1b.yaml --bench_cfg configs/bench/infer_decoder_scaling.yaml --output_dir results
	# 9. Llama 3.2 3B
	@echo "9. Benchmarking Llama 3.2 3B scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/llama-3.2-3b.yaml --bench_cfg configs/bench/infer_decoder_scaling.yaml --output_dir results
	# 10. Qwen2.5 0.5B
	@echo "10. Benchmarking Qwen2.5 0.5B scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/qwen2.5-0.5b.yaml --bench_cfg configs/bench/infer_decoder_scaling.yaml --output_dir results
	# 11. Qwen2.5 1.5B
	@echo "11. Benchmarking Qwen2.5 1.5B scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/qwen2.5-1.5b.yaml --bench_cfg configs/bench/infer_decoder_scaling.yaml --output_dir results
	# 12. BERT Base Uncased
	@echo "12. Benchmarking BERT Base Uncased scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/bert-base.yaml --bench_cfg configs/bench/infer_encoder_scaling.yaml --output_dir results
	# 13. DistilBERT Base
	@echo "13. Benchmarking DistilBERT Base scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/distilbert-base.yaml --bench_cfg configs/bench/infer_encoder_scaling.yaml --output_dir results
	# 14. ModernBERT Base
	@echo "14. Benchmarking ModernBERT Base scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/modernbert-base.yaml --bench_cfg configs/bench/infer_encoder_scaling.yaml --output_dir results
	# 15. RoBERTa Base
	@echo "15. Benchmarking RoBERTa Base scaling..."
	python -m src.e3bench.eval.bench_infer --model_cfg configs/model/roberta-base.yaml --bench_cfg configs/bench/infer_encoder_scaling.yaml --output_dir results
	@echo "Done! All scaling benchmarks completed."

