{
  "exp_id": "fewshot_20251129_225120",
  "commit": "067ece39",
  "config": {
    "model": {
      "name": "qwen2.5-1.5b",
      "arch": "decoder",
      "pretrained": "Qwen/Qwen2.5-1.5B",
      "dtype": "fp16",
      "attn_impl": "sdpa",
      "use_lora": true,
      "max_length": 4096
    },
    "eval": {
      "name": "fewshot_5",
      "num_fewshot": 5,
      "max_length": 512,
      "batch_size": 1,
      "limit": 100,
      "tasks": [
        "mmlu"
      ]
    }
  },
  "metrics": {
    "mmlu_accuracy": 0.6235087719298246,
    "mmlu_humanities_accuracy": 0.6484615384615384,
    "mmlu_formal_logic_accuracy": 0.44,
    "mmlu_high_school_european_history_accuracy": 0.71,
    "mmlu_high_school_us_history_accuracy": 0.73,
    "mmlu_high_school_world_history_accuracy": 0.75,
    "mmlu_international_law_accuracy": 0.8,
    "mmlu_jurisprudence_accuracy": 0.79,
    "mmlu_logical_fallacies_accuracy": 0.74,
    "mmlu_moral_disputes_accuracy": 0.62,
    "mmlu_moral_scenarios_accuracy": 0.26,
    "mmlu_philosophy_accuracy": 0.71,
    "mmlu_prehistory_accuracy": 0.7,
    "mmlu_professional_law_accuracy": 0.4,
    "mmlu_world_religions_accuracy": 0.78,
    "mmlu_other_accuracy": 0.6484615384615384,
    "mmlu_business_ethics_accuracy": 0.69,
    "mmlu_clinical_knowledge_accuracy": 0.66,
    "mmlu_college_medicine_accuracy": 0.7,
    "mmlu_global_facts_accuracy": 0.37,
    "mmlu_human_aging_accuracy": 0.63,
    "mmlu_management_accuracy": 0.8,
    "mmlu_marketing_accuracy": 0.9,
    "mmlu_medical_genetics_accuracy": 0.69,
    "mmlu_miscellaneous_accuracy": 0.71,
    "mmlu_nutrition_accuracy": 0.71,
    "mmlu_professional_accounting_accuracy": 0.5,
    "mmlu_professional_medicine_accuracy": 0.59,
    "mmlu_virology_accuracy": 0.48,
    "mmlu_social_sciences_accuracy": 0.7116666666666667,
    "mmlu_econometrics_accuracy": 0.46,
    "mmlu_high_school_geography_accuracy": 0.8,
    "mmlu_high_school_government_and_politics_accuracy": 0.81,
    "mmlu_high_school_macroeconomics_accuracy": 0.74,
    "mmlu_high_school_microeconomics_accuracy": 0.65,
    "mmlu_high_school_psychology_accuracy": 0.81,
    "mmlu_human_sexuality_accuracy": 0.76,
    "mmlu_professional_psychology_accuracy": 0.63,
    "mmlu_public_relations_accuracy": 0.64,
    "mmlu_security_studies_accuracy": 0.69,
    "mmlu_sociology_accuracy": 0.75,
    "mmlu_us_foreign_policy_accuracy": 0.8,
    "mmlu_stem_accuracy": 0.5336842105263158,
    "mmlu_abstract_algebra_accuracy": 0.33,
    "mmlu_anatomy_accuracy": 0.52,
    "mmlu_astronomy_accuracy": 0.72,
    "mmlu_college_biology_accuracy": 0.69,
    "mmlu_college_chemistry_accuracy": 0.43,
    "mmlu_college_computer_science_accuracy": 0.51,
    "mmlu_college_mathematics_accuracy": 0.39,
    "mmlu_college_physics_accuracy": 0.46,
    "mmlu_computer_security_accuracy": 0.78,
    "mmlu_conceptual_physics_accuracy": 0.61,
    "mmlu_electrical_engineering_accuracy": 0.7,
    "mmlu_elementary_mathematics_accuracy": 0.47,
    "mmlu_high_school_biology_accuracy": 0.75,
    "mmlu_high_school_chemistry_accuracy": 0.52,
    "mmlu_high_school_computer_science_accuracy": 0.65,
    "mmlu_high_school_mathematics_accuracy": 0.41,
    "mmlu_high_school_physics_accuracy": 0.34,
    "mmlu_high_school_statistics_accuracy": 0.49,
    "mmlu_machine_learning_accuracy": 0.37
  },
  "timing": {
    "start_time": 1764478280.3504472,
    "end_time": 1764479528.348495,
    "duration_seconds": 1247.9980478286743
  },
  "resources": {
    "gpu_name": "Tesla V100-SXM2-32GB",
    "max_memory_gb": 0.0,
    "cuda_version": "12.8"
  },
  "power": {
    "avg_watt": null,
    "kwh": null,
    "duration_seconds": 1250.4660341739655
  },
  "timestamp": 1764479530.821101
}