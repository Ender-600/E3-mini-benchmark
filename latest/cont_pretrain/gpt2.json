{
  "exp_id": "cont_pretrain_20251110_203350",
  "commit": "d557d298",
  "config": {
    "model": {
      "name": "gpt2",
      "arch": "decoder",
      "pretrained": "gpt2",
      "dtype": "fp16",
      "attn_impl": "eager",
      "use_lora": true,
      "max_length": 256
    },
    "train": {
      "name": "lora",
      "fp16": false,
      "grad_checkpointing": false,
      "per_device_train_batch_size": 16,
      "per_device_eval_batch_size": 32,
      "gradient_accumulation_steps": 2,
      "num_train_epochs": 5,
      "learning_rate": 5e-05,
      "weight_decay": 0.01,
      "warmup_ratio": 0.1,
      "max_length": 256,
      "seed": [
        42
      ],
      "save_strategy": "epoch",
      "eval_strategy": "epoch",
      "logging_steps": 10,
      "save_total_limit": 2,
      "lora_r": 16,
      "lora_alpha": 32,
      "lora_dropout": 0.05
    },
    "dataset": "wikitext",
    "target_loss": 2.0,
    "token_budget": 1000000
  },
  "metrics": {
    "final_eval_loss": 3.7754969596862793,
    "best_eval_loss": 3.7754969596862793,
    "epochs_trained": 5,
    "total_tokens": 661760,
    "token_budget": 1000000,
    "duration_seconds": 136.20588994026184,
    "tokens_per_second": 4858.527045271239,
    "target_reached": false,
    "lora_params": {
      "trainable_params": 1622016,
      "total_params": 126061824,
      "trainable_percentage": 1.2866829532785438
    }
  },
  "timing": {
    "start_time": 1762828430.246258,
    "end_time": 1762828566.4541497,
    "duration_seconds": 136.20789170265198
  },
  "resources": {
    "gpu_name": "Tesla V100-SXM2-32GB",
    "max_memory_gb": 11.91006326675415,
    "cuda_version": "12.8"
  },
  "power": {
    "avg_watt": 613.9540000000001,
    "kwh": 0.023231628445407815,
    "duration_seconds": 136.22170782089233
  },
  "timestamp": 1762828566.4703262
}